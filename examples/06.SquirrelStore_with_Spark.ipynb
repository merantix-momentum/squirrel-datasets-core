{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fb0a7b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/merantix-momentum/squirrel-datasets-core/blob/main/examples/06.SquirrelStore_with_Spark.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import squirrel\n",
    "    import squirrel_datasets_core\n",
    "    import pyspark\n",
    "except:\n",
    "    !pip install -q --ignore-requires-python --upgrade squirrel-datasets-core pyspark # noqa\n",
    "    import squirrel\n",
    "    import squirrel_datasets_core\n",
    "    import pyspark\n",
    "\n",
    "print(squirrel.__version__)\n",
    "print(squirrel_datasets_core.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cc372",
   "metadata": {},
   "source": [
    "If you run this tutorial in google colab, there unfortunately are some tweaks we need to apply to make it work - we will skip this step automatically if we cannot detect colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    if restart_colab:\n",
    "        !pip install -q --upgrade --force pyyaml==5.4.1\n",
    "\n",
    "        # need to restart kernel\n",
    "        import os\n",
    "\n",
    "        os.kill(os.getpid(), 9)\n",
    "except:\n",
    "    # not in colab\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba755167",
   "metadata": {},
   "source": [
    "If you have not already, refer to the documentation page for `Store` to cover the basics first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca45c2c",
   "metadata": {},
   "source": [
    "## Reading and writing to the store using Spark\n",
    "\n",
    "Squirrel makes it a breeze to scale out any data workload.\n",
    "To illustrate this using Spark, we:\n",
    "\n",
    "1. Create a data source\n",
    "\n",
    "2. Initialize a Driver that can read from the data source\n",
    "\n",
    "3. Construct an RDD in Spark from the data loaded using the Driver\n",
    "\n",
    "4. Create a DataFrame from the RDD and write data into shards using SquirrelStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96805124",
   "metadata": {},
   "source": [
    "Let's first create some dummy data and save it into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04070b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "N_SAMPLES = 1_000\n",
    "\n",
    "\n",
    "def create_sample():\n",
    "    return {\n",
    "        \"name\": np.random.choice([\"John\", \"Jane\"]),\n",
    "        \"identifier\": int(np.random.choice([1, 2])),\n",
    "        \"age\": int(np.random.choice([20, 30])),\n",
    "    }\n",
    "\n",
    "\n",
    "samples = [create_sample() for _ in range(N_SAMPLES)]\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "csv_path = f\"{tmpdir.name}/my_source.csv\"\n",
    "pd.DataFrame(samples).to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0b27a",
   "metadata": {},
   "source": [
    "Now, we can read the source using the CsvDriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squirrel.driver import CsvDriver\n",
    "\n",
    "driver = CsvDriver(csv_path)\n",
    "df = pd.DataFrame(driver.get_iter().collect()).set_index(\"Index\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913c5ed",
   "metadata": {},
   "source": [
    "We will convert the data into an RDD first.\n",
    "For this we need to provide a schema.\n",
    "Note that the schema is the only data-source-specific part in the pipeline.\n",
    "As long as we provide the correct schema, we can use the pipeline with any driver or store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"identifier\", StringType(), False),\n",
    "        StructField(\"age\", IntegerType(), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "parallel_collection_rdd = spark.sparkContext.parallelize(driver.get_iter())\n",
    "df = spark.createDataFrame(parallel_collection_rdd, SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864f9aa",
   "metadata": {},
   "source": [
    "RDD is ready.\n",
    "Now we can write the data into shards.\n",
    "We opt for the SquirrelStore that is used by the MessagepackDriver here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from squirrel.serialization import MessagepackSerializer\n",
    "from squirrel.store import SquirrelStore\n",
    "\n",
    "\n",
    "def save_iterable_as_shard(it, url) -> None:\n",
    "    \"\"\"Helper to save a shard into a messagepack store using squirrel.\"\"\"\n",
    "    SquirrelStore(url, serializer=MessagepackSerializer()).set(value=list(it))\n",
    "\n",
    "\n",
    "tmpdir2 = tempfile.TemporaryDirectory()\n",
    "N_SHARDS = 10\n",
    "\n",
    "_ = (\n",
    "    df.rdd.map(lambda row: row.asDict())\n",
    "    .repartition(N_SHARDS)\n",
    "    .foreachPartition(partial(save_iterable_as_shard, url=tmpdir2.name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b150640",
   "metadata": {},
   "source": [
    "# Reading big SquirrelStores into Spark\n",
    "\n",
    "When loading big datasets, the call `spark.sparkContext.parallelize(driver.get_iter())` will not work anymore because spark driver will tries to distribute the data. Instead, we should distribute the keys and only load the data on the executors. Squirrel makes it easy to achieve this thanks to the design of `SquirrelStore` which exposes a `keys()` and `get()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_sample(x):\n",
    "    return {\"img\": np.random.random((30, 30, 3)), \"label\": x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squirrel.iterstream import IterableSource\n",
    "\n",
    "tmpdir3 = tempfile.TemporaryDirectory()\n",
    "N_SHARDS = 10\n",
    "\n",
    "store = SquirrelStore(tmpdir3.name, serializer=MessagepackSerializer())\n",
    "IterableSource(range(100)).map(create_image_sample).batched(10).map(store.set).join()  # save some data in the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = (\n",
    "    spark.sparkContext.parallelize(store.keys())  # distribute keys between spark executors\n",
    "    .map(lambda key: list(store.get(key)))  # load the shard on each executor\n",
    "    .flatMap(lambda x: x)  # flattens each shard to get individual items\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150adb08",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()\n",
    "tmpdir2.cleanup()\n",
    "tmpdir3.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7feb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
